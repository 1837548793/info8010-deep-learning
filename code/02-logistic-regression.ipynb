{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from [Pytorch tutorial for Deep Learning researchers](https://github.com/yunjey/pytorch-tutorial) (Yunvey Choi, 2018).\n",
    "\n",
    "Used as part of Deep Learning, Gilles Louppe, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels)\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Dataset Loader (Input Pipline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader)\n",
    "X, y = data_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8eff484cc0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model = LogisticRegression(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1455  0.1905  0.4963  ...  -0.0244 -0.1913 -0.0413\n",
       " 0.1488  0.0278  0.0278  ...  -0.1386 -0.1633 -0.1846\n",
       " 0.2674  0.0858 -0.1752  ...  -0.1133  0.1344  0.0985\n",
       "          ...             â‹±             ...          \n",
       " 0.2992  0.0278 -0.0148  ...  -0.2400  0.1752  0.0147\n",
       " 0.1983  0.1622  0.3954  ...  -0.1230 -0.3849  0.2058\n",
       " 0.3478 -0.3198  0.3208  ...  -0.0655 -0.0109  0.1070\n",
       "[torch.FloatTensor of size 100x10]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(Variable(X.view(-1, 28*28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [100/600], Loss: 2.1950\n",
      "Epoch: [1/5], Step: [200/600], Loss: 2.0979\n",
      "Epoch: [1/5], Step: [300/600], Loss: 1.9969\n",
      "Epoch: [1/5], Step: [400/600], Loss: 1.9561\n",
      "Epoch: [1/5], Step: [500/600], Loss: 1.8602\n",
      "Epoch: [1/5], Step: [600/600], Loss: 1.7706\n",
      "Epoch: [2/5], Step: [100/600], Loss: 1.7583\n",
      "Epoch: [2/5], Step: [200/600], Loss: 1.6145\n",
      "Epoch: [2/5], Step: [300/600], Loss: 1.5692\n",
      "Epoch: [2/5], Step: [400/600], Loss: 1.6344\n",
      "Epoch: [2/5], Step: [500/600], Loss: 1.5381\n",
      "Epoch: [2/5], Step: [600/600], Loss: 1.4548\n",
      "Epoch: [3/5], Step: [100/600], Loss: 1.4436\n",
      "Epoch: [3/5], Step: [200/600], Loss: 1.4030\n",
      "Epoch: [3/5], Step: [300/600], Loss: 1.4383\n",
      "Epoch: [3/5], Step: [400/600], Loss: 1.2643\n",
      "Epoch: [3/5], Step: [500/600], Loss: 1.2676\n",
      "Epoch: [3/5], Step: [600/600], Loss: 1.2026\n",
      "Epoch: [4/5], Step: [100/600], Loss: 1.1776\n",
      "Epoch: [4/5], Step: [200/600], Loss: 1.2393\n",
      "Epoch: [4/5], Step: [300/600], Loss: 1.1839\n",
      "Epoch: [4/5], Step: [400/600], Loss: 1.2077\n",
      "Epoch: [4/5], Step: [500/600], Loss: 1.2439\n",
      "Epoch: [4/5], Step: [600/600], Loss: 1.1694\n",
      "Epoch: [5/5], Step: [100/600], Loss: 1.0303\n",
      "Epoch: [5/5], Step: [200/600], Loss: 1.0790\n",
      "Epoch: [5/5], Step: [300/600], Loss: 0.9840\n",
      "Epoch: [5/5], Step: [400/600], Loss: 1.0801\n",
      "Epoch: [5/5], Step: [500/600], Loss: 1.1351\n",
      "Epoch: [5/5], Step: [600/600], Loss: 0.9443\n"
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated.\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n",
    "                  % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 82 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
