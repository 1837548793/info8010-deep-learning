{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from [Pytorch tutorial for Deep Learning researchers](https://github.com/yunjey/pytorch-tutorial) (Yunvey Choi, 2018).\n",
    "\n",
    "Used as part of Deep Learning, Gilles Louppe, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels)\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Dataset Loader (Input Pipline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader)\n",
    "X, y = data_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc98f2e1b00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model = LogisticRegression(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.4460  0.0141  0.0909  ...   0.0808 -0.0439  0.2126\n",
       "-0.3079  0.0216  0.3513  ...   0.0560 -0.2041  0.0670\n",
       "-0.1486  0.0020  0.2362  ...  -0.1650 -0.4598 -0.1179\n",
       "          ...             â‹±             ...          \n",
       "-0.2045 -0.0096  0.1602  ...  -0.2503 -0.2923  0.3095\n",
       "-0.2357 -0.0075 -0.0587  ...  -0.0564 -0.1246  0.3168\n",
       " 0.3288  0.1113  0.0012  ...   0.0592 -0.0484  0.1691\n",
       "[torch.FloatTensor of size 100x10]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(Variable(X.view(-1, 28*28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [100/600], Loss: 2.2039\n",
      "Epoch: [1/5], Step: [200/600], Loss: 2.0976\n",
      "Epoch: [1/5], Step: [300/600], Loss: 2.0135\n",
      "Epoch: [1/5], Step: [400/600], Loss: 1.9435\n",
      "Epoch: [1/5], Step: [500/600], Loss: 1.8512\n",
      "Epoch: [1/5], Step: [600/600], Loss: 1.7444\n",
      "Epoch: [2/5], Step: [100/600], Loss: 1.7006\n",
      "Epoch: [2/5], Step: [200/600], Loss: 1.6798\n",
      "Epoch: [2/5], Step: [300/600], Loss: 1.6229\n",
      "Epoch: [2/5], Step: [400/600], Loss: 1.5971\n",
      "Epoch: [2/5], Step: [500/600], Loss: 1.4596\n",
      "Epoch: [2/5], Step: [600/600], Loss: 1.5359\n",
      "Epoch: [3/5], Step: [100/600], Loss: 1.4093\n",
      "Epoch: [3/5], Step: [200/600], Loss: 1.3769\n",
      "Epoch: [3/5], Step: [300/600], Loss: 1.4319\n",
      "Epoch: [3/5], Step: [400/600], Loss: 1.3070\n",
      "Epoch: [3/5], Step: [500/600], Loss: 1.2769\n",
      "Epoch: [3/5], Step: [600/600], Loss: 1.2769\n",
      "Epoch: [4/5], Step: [100/600], Loss: 1.2161\n",
      "Epoch: [4/5], Step: [200/600], Loss: 1.0847\n",
      "Epoch: [4/5], Step: [300/600], Loss: 1.1381\n",
      "Epoch: [4/5], Step: [400/600], Loss: 1.2160\n",
      "Epoch: [4/5], Step: [500/600], Loss: 1.1197\n",
      "Epoch: [4/5], Step: [600/600], Loss: 1.1232\n",
      "Epoch: [5/5], Step: [100/600], Loss: 1.0204\n",
      "Epoch: [5/5], Step: [200/600], Loss: 1.1470\n",
      "Epoch: [5/5], Step: [300/600], Loss: 1.1118\n",
      "Epoch: [5/5], Step: [400/600], Loss: 1.1397\n",
      "Epoch: [5/5], Step: [500/600], Loss: 1.0003\n",
      "Epoch: [5/5], Step: [600/600], Loss: 1.0386\n"
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated.\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n",
    "                  % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 82 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
